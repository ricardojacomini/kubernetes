# roles/kubernetes/tasks/master.yml
---
- name: Set local Kubernetes version
  set_fact:
    k8s_version: "{{ versions.kubernetes }}"

- name: Print Kubernetes version
  debug:
    msg: "Using Kubernetes version: {{ k8s_version }}"

- name: Install Kubernetes packages
  package:
    name:
      - kubectl-{{ k8s_version }}
      - kubelet-{{ k8s_version }}
      - kubeadm-{{ k8s_version }}
      - kubernetes-cni-{{ versions.kubernetes_cni }}
    state: present

# ✅ Ensure kubelet service is available before triggering any handlers
- name: Ensure kubelet is installed
  package:
    name: kubelet
    state: present

- name: Re-exec systemd
  command: systemctl daemon-reexec
  changed_when: false

- name: Reload systemd
  command: systemctl daemon-reload
  changed_when: false

- name: Gather service facts
  service_facts:

- name: Ensure kubelet service is enabled and started
  systemd:
    name: kubelet
    enabled: yes
    state: started
  when: "'kubelet.service' in ansible_facts.services"

- name: "🔍 DEBUG: Confirm handler is loaded (Restart Kubelet)"
  debug:
    msg: "Handler will trigger successfully"
  notify: "Restart Kubelet"

- name: "🔄 Configure kubelet"
  template:
    src: kubelet.conf.j2
    dest: /etc/sysconfig/kubelet
    owner: root
    group: root
    mode: 0644
  notify: "🔄 Restart Kubelet"

- name: "🛡️ Initialize Kubernetes Control Plane"
  become: true
  command: >
    kubeadm init
    --kubernetes-version={{ k8s_version }}
    --pod-network-cidr={{ pod_network_cidr }}
    --service-cidr={{ service_cidr }}
    --control-plane-endpoint={{ k8s_api_ip }}
    --apiserver-advertise-address={{ k8s_api_ip }}
    --token-ttl=1h
    --upload-certs
    --apiserver-cert-extra-sans={{ k8s_api_ip }}
    --ignore-preflight-errors=Swap,SystemVerification
  register: kubeadm_init
  changed_when: "'Your Kubernetes control-plane has initialized' in kubeadm_init.stdout"
  args:
    creates: /etc/kubernetes/admin.conf

- name: "🔗 Set kubectl context for cluster administration"
  block:

    - name: "📍 Determine kubeconfig directory"
      set_fact:
        kubeconfig_dir: "{{ '/root/.kube' if ansible_user == 'root' else '/home/' + ansible_user + '/.kube' }}"

    - name: "👥 Get primary group of the user"
      command: id -gn {{ ansible_user }}
      register: user_primary_group
      changed_when: false

    - name: "📁 Ensure .kube directory exists"
      file:
        path: "{{ kubeconfig_dir }}"
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ user_primary_group.stdout }}"
        mode: '0755'

    - name: "🔍 Check if admin.conf exists"
      stat:
        path: /etc/kubernetes/admin.conf
      register: admin_conf_file

    - name: "🛠️ Replace server address in admin.conf with control plane IP"
      command: >
        kubectl config set-cluster kubernetes
        --server=https://{{ k8s_api_ip }}:6443
        --kubeconfig=/etc/kubernetes/admin.conf
      when:
        - k8s_api_ip is defined
        - admin_conf_file.stat.exists

    - name: "🛠️ Replace server in user kubeconfig"
      command: >
        kubectl config set-cluster kubernetes
        --server=https://{{ k8s_api_ip }}:6443
        --kubeconfig={{ kubeconfig_dir }}/config
      when:
        - k8s_api_ip is defined
        - admin_conf_file.stat.exists

    - name: "🔑 Generate kubeadm join command"
      command: kubeadm token create --print-join-command --ttl 30m --certificate-key $(kubeadm init phase upload-certs --upload-certs | tail -n1)
      register: kubeadm_join_cmd
      when: kubeadm_init is changed

    - name: "📝 Save kubeadm join command"
      copy:
        content: "{{ kubeadm_join_cmd.stdout }}"
        dest: "{{ kubeconfig_dir }}/join-command.sh"
        mode: '0755'
      when: kubeadm_join_cmd.stdout is defined

    - name: "Check if Flannel manifest exists"
      stat:
        path: /opt/k8s/manifests/kube-flannel.yml
      register: flannel_manifest

    - name: 📦 Deploy Flannel CNI from local file
      command: kubectl apply -f /opt/k8s/manifests/kube-flannel.yml
      when:
        - kubeadm_init is changed               # ✅ Only applies Flannel on fresh control plane init
        - flannel_manifest.stat.exists          # ✅ Ensures the file actually exists
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf  # ✅ Uses correct kubeconfig for root/admin context
      tags: [kubernetes, flannel, cni]

    - name: ✅ Wait for Flannel CNI pod to be ready
      shell: >
        kubectl get pods -n kube-system -l app=flannel -o jsonpath='{.items[*].status.containerStatuses[*].ready}' | grep -q true
      register: flannel_ready
      retries: 10
      delay: 10
      until: flannel_ready.rc == 0
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when:
        - kubeadm_init is changed
        - flannel_manifest.stat.exists
      tags: [kubernetes, flannel, cni]

    # RBAC Binding via admin.conf
    - name: 🛡️ Ensure kubernetes-admin has cluster-admin privileges
      command: >
        kubectl create clusterrolebinding kubernetes-admin-binding
        --clusterrole=cluster-admin
        --user=kubernetes-admin
      register: admin_binding
      failed_when: admin_binding.rc != 0 and "'AlreadyExists' not in admin_binding.stderr"
      changed_when: "'created' in admin_binding.stdout"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when:
        - kubeadm_init is changed
        - flannel_ready is defined and flannel_ready.rc == 0
      tags: [kubernetes, admin]

# - name: Install Pod network
#   command: kubectl apply -f "{{ network_plugins[network_plugin].manifest }}"
#   when: kubeadm_init is changed
#   register: network_install
#   changed_when: "'created' in network_install.stdout or 'configured' in network_install.stdout"

- name: 📝 Create fallback ClusterRoleBinding manifest for kubernetes-admin
  copy:
    dest: /etc/kubernetes/admin-clusterrolebinding.yaml
    content: |
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: kubernetes-admin
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: cluster-admin
      subjects:
      - kind: User
        name: kubernetes-admin
        apiGroup: rbac.authorization.k8s.io
    owner: root
    group: root
    mode: '0644'
  tags: [kubernetes, admin, recovery]
  when: kubeadm_init is changed

- name: ✅ Validate control plane is responsive
  command: kubectl get nodes
  register: kubectl_nodes
  changed_when: false
  retries: 5
  delay: 10
  until: kubectl_nodes.rc == 0
  environment:
    KUBECONFIG: "{{ kubeconfig_dir }}/config"
  when: kubeadm_init is changed

- name: 📁 Override server in user kubeconfig
  command: >
    kubectl config set-cluster kubernetes
    --server=https://{{ k8s_api_ip }}:6443
    --kubeconfig={{ kubeconfig_dir }}/config
  when:
    - k8s_api_ip is defined

- name: 🛡️ Fix ownership of kubeconfig
  file:
    path: "{{ kubeconfig_dir }}/config"
    owner: "{{ ansible_user }}"
    group: "{{ user_primary_group.stdout }}"
    mode: '0600'
  when:
    - k8s_api_ip is defined

- name: 🔐 Add user to root group for containerd access
  user:
    name: "{{ ansible_user }}"
    groups: root
    append: yes
  become: true
  tags: containerd